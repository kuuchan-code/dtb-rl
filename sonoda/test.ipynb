{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オリジナルのCartpoleを継承\n",
    "class MyCartpole(CartPoleEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # action_space を連続空間に変更\n",
    "        self.action_space = spaces.Box(-self.force_mag, self.force_mag, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # 終了ターンのが実装内で定義されているので実装\n",
    "        self.step_count = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # 200ターンたったら終了する\n",
    "        self.step_count += 1\n",
    "        if self.step_count > 200:\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # 例外処理\n",
    "        if np.isnan(action):\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # アクションをclipして force の値にする\n",
    "        force = np.clip(action, -self.force_mag, self.force_mag)[0]\n",
    "\n",
    "        #--- 以下オリジナルのstepコードをコピペ ---\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24, reward: 24.0\n"
     ]
    }
   ],
   "source": [
    "env = MyCartpole()\n",
    "# env = gnwrapper.LoopAnimation(env)\n",
    "\n",
    "env.reset()     # ゲームの初期化\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "env.close()\n",
    "\n",
    "print(\"step: {}, reward: {}\".format(step, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 独自のモデルを定義\n",
    "class PolicyModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.pi_mean = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "        self.pi_stddev = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "\n",
    "        # optimizer もついでに定義しておく\n",
    "        self.optimizer = Adam(lr=0.01)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        # σ^2 > 0 になるように変換(指数関数)\n",
    "        stddev = tf.exp(stddev)\n",
    "\n",
    "        return mean, stddev\n",
    "\n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state):\n",
    "        # モデルから平均と分散を取得\n",
    "        mean, stddev = self(state.reshape((1,-1)))\n",
    "\n",
    "        # ガウス分布に従った乱数をだす\n",
    "        sampled_action = tf.random.normal(tf.shape(mean), mean=mean, stddev=stddev)\n",
    "        return sampled_action.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
