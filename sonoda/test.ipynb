{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.8/dist-packages (1.13.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (4.4.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (3.19.4)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.2.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from ray[rllib]) (2.22.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (21.4.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.43.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.22.1)\n",
      "Requirement already satisfied: virtualenv in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (20.14.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.0.4)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (8.0.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (3.7.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.3.0)\n",
      "Requirement already satisfied: gym<0.22 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (0.21.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (2.5.1)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (0.19.3)\n",
      "Requirement already satisfied: lz4 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (4.0.1)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (0.1.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.4.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (0.8.10)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (1.8.1)\n",
      "Requirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.8/dist-packages (from ray[rllib]) (3.5.1)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray[rllib]) (1.14.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym<0.22->ray[rllib]) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (9.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (1.3.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[rllib]) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray[rllib]) (5.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->ray[rllib]) (2022.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]) (2.19.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]) (2022.5.4)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image->ray[rllib]) (2.8.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from virtualenv->ray[rllib]) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.8/dist-packages (from virtualenv->ray[rllib]) (2.4.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray[rllib]) (3.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"ray[rllib]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 10:06:51,212\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-06-28 10:06:52,811\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67096576 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.82gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-06-28 10:07:01,677\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-06-28 10:07:01,677\tWARNING trainer.py:2540 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-06-28 10:07:01,739\tINFO trainable.py:159 -- Trainable.setup took 10.528 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-28 10:07:01,740\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-06-28 10:07:03,146\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 9.953198, 'policy_loss': 0.008731895, 'vf_loss': 9.944329, 'vf_explained_var': -8.963833e-06, 'kl': 0.0006804432, 'entropy': 1.7910774, 'entropy_coeff': 0.0, 'model': {}}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': -414.0, 'episode_reward_min': -983.0, 'episode_reward_mean': -783.1, 'episode_len_mean': 196.0, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-929.0, -758.0, -830.0, -731.0, -785.0, -650.0, -812.0, -414.0, -749.0, -839.0, -794.0, -803.0, -821.0, -821.0, -785.0, -749.0, -722.0, -857.0, -830.0, -983.0], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 120, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.07624008964145855, 'mean_inference_ms': 0.5054445281021599, 'mean_action_processing_ms': 0.050442448739466926, 'mean_env_wait_ms': 0.043035149276405504, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}}, 'episode_reward_max': -414.0, 'episode_reward_min': -983.0, 'episode_reward_mean': -783.1, 'episode_len_mean': 196.0, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-929.0, -758.0, -830.0, -731.0, -785.0, -650.0, -812.0, -414.0, -749.0, -839.0, -794.0, -803.0, -821.0, -821.0, -785.0, -749.0, -722.0, -857.0, -830.0, -983.0], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 120, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.07624008964145855, 'mean_inference_ms': 0.5054445281021599, 'mean_action_processing_ms': 0.050442448739466926, 'mean_env_wait_ms': 0.043035149276405504, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_healthy_workers': 2, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'timesteps_total': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 3993.349, 'load_time_ms': 0.183, 'load_throughput': 21873814.863, 'learn_time_ms': 2578.749, 'learn_throughput': 1551.139, 'update_time_ms': 1.826}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'episodes_total': 20, 'training_iteration': 1, 'trial_id': 'default', 'experiment_id': '794999d991614efa8e5a5cee93079861', 'date': '2022-06-28_10-07-05', 'timestamp': 1656410825, 'time_this_iter_s': 3.995251417160034, 'time_total_s': 3.995251417160034, 'pid': 998, 'hostname': 'c0aa52e2ede1', 'node_ip': '172.17.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'relu', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Taxi-v3', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': 0, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 1, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'relu', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Taxi-v3', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': True, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': 0, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': True, 'evaluation_config': {'render_env': True}, 'evaluation_num_workers': 1, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'keep_per_episode_custom_metrics': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Discrete(500), action_space=Discrete(6), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'disable_env_checking': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'lambda': 1.0}, 'evaluation_num_workers': 1, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'keep_per_episode_custom_metrics': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Discrete(500), action_space=Discrete(6), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'disable_env_checking': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'lambda': 1.0}, 'time_since_restore': 3.995251417160034, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 10.529680490493774, 'perf': {'cpu_util_percent': 25.933333333333334, 'ram_util_percent': 45.56666666666666}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612, 'cur_lr': 4.999999873689376e-05, 'total_loss': 9.893939, 'policy_loss': 0.004841795, 'vf_loss': 9.888768, 'vf_explained_var': -0.00011243378, 'kl': 0.0033034717, 'entropy': 1.7833376, 'entropy_coeff': 0.0, 'model': {}}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000}, 'sampler_results': {'episode_reward_max': -123.0, 'episode_reward_min': -983.0, 'episode_reward_mean': -736.475, 'episode_len_mean': 193.55, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-929.0, -758.0, -830.0, -731.0, -785.0, -650.0, -812.0, -414.0, -749.0, -839.0, -794.0, -803.0, -821.0, -821.0, -785.0, -749.0, -722.0, -857.0, -830.0, -983.0, -713.0, -749.0, -776.0, -668.0, -794.0, -740.0, -686.0, -749.0, -803.0, -713.0, -695.0, -623.0, -123.0, -552.0, -740.0, -704.0, -794.0, -686.0, -677.0, -812.0], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 120, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 45, 177, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.07601197335837026, 'mean_inference_ms': 0.49754084160655065, 'mean_action_processing_ms': 0.04977884453004679, 'mean_env_wait_ms': 0.04255726937207391, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}}, 'episode_reward_max': -123.0, 'episode_reward_min': -983.0, 'episode_reward_mean': -736.475, 'episode_len_mean': 193.55, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-929.0, -758.0, -830.0, -731.0, -785.0, -650.0, -812.0, -414.0, -749.0, -839.0, -794.0, -803.0, -821.0, -821.0, -785.0, -749.0, -722.0, -857.0, -830.0, -983.0, -713.0, -749.0, -776.0, -668.0, -794.0, -740.0, -686.0, -749.0, -803.0, -713.0, -695.0, -623.0, -123.0, -552.0, -740.0, -704.0, -794.0, -686.0, -677.0, -812.0], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 120, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 45, 177, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.07601197335837026, 'mean_inference_ms': 0.49754084160655065, 'mean_action_processing_ms': 0.04977884453004679, 'mean_env_wait_ms': 0.04255726937207391, 'mean_env_render_ms': 0.0}, 'off_policy_estimator': {}, 'num_healthy_workers': 2, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'timesteps_total': 8000, 'agent_timesteps_total': 8000, 'timers': {'training_iteration_time_ms': 3774.9, 'load_time_ms': 0.178, 'load_throughput': 22444436.12, 'learn_time_ms': 2362.846, 'learn_throughput': 1692.874, 'update_time_ms': 1.834}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000}, 'done': False, 'episodes_total': 40, 'training_iteration': 2, 'trial_id': 'default', 'experiment_id': '794999d991614efa8e5a5cee93079861', 'date': '2022-06-28_10-07-09', 'timestamp': 1656410829, 'time_this_iter_s': 3.559248685836792, 'time_total_s': 7.554500102996826, 'pid': 998, 'hostname': 'c0aa52e2ede1', 'node_ip': '172.17.0.2', 'config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'relu', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Taxi-v3', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': 0, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 1, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'relu', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Taxi-v3', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': True, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': 0, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': True, 'evaluation_config': {'render_env': True}, 'evaluation_num_workers': 1, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'keep_per_episode_custom_metrics': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Discrete(500), action_space=Discrete(6), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'disable_env_checking': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'lambda': 1.0}, 'evaluation_num_workers': 1, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'keep_per_episode_custom_metrics': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Discrete(500), action_space=Discrete(6), config={})}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'disable_env_checking': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'lambda': 1.0}, 'time_since_restore': 7.554500102996826, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'warmup_time': 10.529680490493774, 'perf': {'cpu_util_percent': 19.1, 'ram_util_percent': 46.220000000000006}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run it for n training iterations. A training iteration includes\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# parallel sample collection by the environment workers as well as\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# loss calculation on the collected batch and a model update.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Evaluate the trained Trainer (and render each timestep to the shell's\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# output).\u001b[39;00m\n\u001b[1;32m     40\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable.py:360\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    359\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_ctx\u001b[38;5;241m.\u001b[39mshould_stop(step_attempt_results):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1112\u001b[0m         step_attempt_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_attempt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# @ray.remote RolloutWorker failure.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;66;03m# Try to recover w/o the failed worker.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer.step_attempt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;66;03m# No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluate_this_iter:\n\u001b[0;32m-> 1214\u001b[0m     step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_plan_or_training_iteration_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# We have to evaluate in this training iteration.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# No parallelism.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/trainer.py:2209\u001b[0m, in \u001b[0;36mTrainer._exec_plan_or_training_iteration_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2209\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2211\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/agents/ppo/ppo.py:437\u001b[0m, in \u001b[0;36mPPOTrainer.training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    434\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers, max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     train_batch \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39mas_multi_agent()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch\u001b[38;5;241m.\u001b[39magent_steps()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/execution/rollout_ops.py:99\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat)\u001b[0m\n\u001b[1;32m     96\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_worker()\u001b[38;5;241m.\u001b[39msample()]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Loop over remote workers' `sample()` method in parallel.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     sample_batches \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update our counters for the stopping criterion of the while loop.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m sample_batches:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:1825\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an object ref \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of object refs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     )\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 1825\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   1827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/worker.py:364\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         )\n\u001b[1;32m    363\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 364\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_ms\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data, metadata) \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import the RL algorithm (Trainer) we would like to use.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(3):\n",
    "    print(trainer.train())\n",
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib train --run APEX --env CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-28 10:07:17,776\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67096576 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.03gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2022-06-28 10:07:19,496\tERROR syncer.py:147 -- Log sync requires rsync to be installed.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:22,884\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:22,885\tINFO simple_q.py:187 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:22,885\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:23 (running for 00:00:04.55)\n",
      "Memory usage on this node: 8.4/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+\n",
      "| Trial name                  | status   | loc             |\n",
      "|-----------------------------+----------+-----------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |\n",
      "+-----------------------------+----------+-----------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:23,837\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:23,904\tWARNING deprecation.py:46 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `RepayBuffer.add()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:23,917\tWARNING multi_agent_prioritized_replay_buffer.py:186 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001b[2m\u001b[36m(DQNTrainer pid=2093)\u001b[0m 2022-06-28 10:07:23,917\tWARNING deprecation.py:46 -- DeprecationWarning: `replay` has been deprecated. Use `sample` instead. This will raise an error in the future!\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:25 (running for 00:00:05.91)\n",
      "Memory usage on this node: 8.4/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |      1 |          1.34146 | 1000 |  20.3061 |                   62 |                    8 |            20.3061 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:30 (running for 00:00:10.99)\n",
      "Memory usage on this node: 8.5/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |      3 |          6.33517 | 3000 |    21.47 |                   88 |                    8 |              21.47 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:35 (running for 00:00:16.30)\n",
      "Memory usage on this node: 8.6/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |      5 |          11.6293 | 5000 |    34.91 |                  200 |                    9 |              34.91 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:42 (running for 00:00:23.69)\n",
      "Memory usage on this node: 8.6/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |      8 |          18.9491 | 8000 |    58.35 |                  200 |                    9 |              58.35 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:50 (running for 00:00:31.05)\n",
      "Memory usage on this node: 8.5/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |     11 |          26.2282 | 11000 |       83 |                  200 |                    9 |                 83 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2022-06-28 10:07:57 (running for 00:00:38.46)\n",
      "Memory usage on this node: 8.6/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |     14 |          33.5751 | 14000 |   106.01 |                  200 |                    9 |             106.01 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:08:02 (running for 00:00:43.53)\n",
      "Memory usage on this node: 8.5/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |     16 |           38.588 | 16000 |   117.56 |                  200 |                   20 |             117.56 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-06-28 10:08:07 (running for 00:00:48.56)\n",
      "Memory usage on this node: 8.6/15.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/12 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc             |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| DQN_CartPole-v0_18300_00000 | RUNNING  | 172.17.0.2:2093 |     18 |          43.5706 | 18000 |   125.13 |                  200 |                   13 |             125.13 |\n",
      "+-----------------------------+----------+-----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!rllib train --run DQN --env CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
