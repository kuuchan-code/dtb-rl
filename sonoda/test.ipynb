{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オリジナルのCartpoleを継承\n",
    "class MyCartpole(CartPoleEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # action_space を連続空間に変更\n",
    "        self.action_space = spaces.Box(-self.force_mag, self.force_mag, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # 終了ターンのが実装内で定義されているので実装\n",
    "        self.step_count = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # デバッグ\n",
    "#         print(self.state, action)\n",
    "        # 200ターンたったら終了する\n",
    "        self.step_count += 1\n",
    "        if self.step_count > 200:\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # 例外処理\n",
    "        if np.isnan(action):\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # アクションをclipして force の値にする\n",
    "        force = np.clip(action, -self.force_mag, self.force_mag)[0]\n",
    "\n",
    "        #--- 以下オリジナルのstepコードをコピペ ---\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25, reward: 25.0\n"
     ]
    }
   ],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "env.reset()     # ゲームの初期化\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "env.close()\n",
    "\n",
    "print(f\"step: {step}, reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 独自のモデルを定義\n",
    "class PolicyModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.pi_mean = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "        self.pi_stddev = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "\n",
    "        # optimizer もついでに定義しておく\n",
    "        self.optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        # σ^2 > 0 になるように変換(指数関数)\n",
    "        stddev = tf.exp(stddev)\n",
    "\n",
    "        return mean, stddev\n",
    "\n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state):\n",
    "        # モデルから平均と分散を取得\n",
    "        mean, stddev = self(state.reshape((1,-1)))\n",
    "\n",
    "        # ガウス分布に従った乱数をだす\n",
    "        sampled_action = tf.random.normal(tf.shape(mean), mean=mean, stddev=stddev)\n",
    "        return sampled_action.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHistory(history_rewards, history_metrics):\n",
    "    # グラフで表示\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('episode')\n",
    "    ax1.grid()\n",
    "    ax1.plot(history_rewards, color=\"C0\", label=\"reward\")\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot([m for m in history_metrics], color=\"C1\", marker='.', label=\"loss\")\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト用?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testEnv(env, sample_action_func):\n",
    "    action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "    action_scale = env.action_space.high - action_centor\n",
    "    for episode in range(5):\n",
    "        state = np.asarray(env.reset())\n",
    "        env.render()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = sample_action_func(state)\n",
    "            n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "            env.render()\n",
    "            state = np.asarray(n_state)\n",
    "            step += 1\n",
    "            total_reward += reward\n",
    "        print(\"{}, step: {}, reward: {}\".format(episode, step, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率分布計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logpi(mean, stddev, action):\n",
    "    a1 = -0.5 * np.log(2*np.pi)\n",
    "    a2 = -tf.math.log(stddev)\n",
    "    a3 = -0.5 * (((action - mean) / stddev) ** 2)\n",
    "    return a1 + a2 + a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, experiences):\n",
    "    gamma = 0.9\n",
    "\n",
    "    # 各経験毎に価値を推定、後ろから計算\n",
    "    v_vals = []\n",
    "    r = 0\n",
    "    for e in reversed(experiences):\n",
    "        if e[\"done\"]:\n",
    "            # 終了時は次の状態がないので報酬のみ\n",
    "            r = e[\"reward\"]\n",
    "        else:\n",
    "            r = e[\"reward\"] + gamma * r\n",
    "        v_vals.append(r)\n",
    "    v_vals.reverse()  # 反転して元に戻す\n",
    "    v_vals = np.asarray(v_vals).reshape((-1, 1))  # 整形\n",
    "\n",
    "    states = np.asarray([e[\"state\"] for e in experiences])\n",
    "    actions = np.asarray([e[\"action\"] for e in experiences])\n",
    "\n",
    "    # baseline\n",
    "    v_vals -= np.mean(v_vals)\n",
    "\n",
    "    # shapeを念のため確認\n",
    "    assert v_vals.shape == (len(experiences), 1)\n",
    "\n",
    "    # 勾配を計算\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # モデルから値を取得\n",
    "        mean, stddev = model(states, training=True)\n",
    "\n",
    "        # log(π(a|s))を計算\n",
    "        logpi = compute_logpi(mean, stddev, actions)\n",
    "\n",
    "        # log(π(a|s)) * Q(s,a) を計算\n",
    "        policy_loss = logpi * v_vals\n",
    "\n",
    "        # ミニバッチ処理\n",
    "        loss = -tf.reduce_mean(policy_loss)\n",
    "\n",
    "    # 勾配を元にoptimizerでモデルを更新\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: reward 57.0 57.0 57.0, loss -0.2 -0.2 -0.2\n",
      "50: reward 13.0 38.2 106.0, loss -0.9 -0.1 0.6\n",
      "100: reward 14.0 52.7 110.0, loss -0.8 -0.1 0.5\n",
      "150: reward 26.0 112.6 200.0, loss -1.3 -0.1 0.6\n",
      "200: reward 103.0 181.6 200.0, loss -0.6 -0.1 0.2\n",
      "250: reward 47.0 167.9 200.0, loss -0.3 0.0 0.2\n",
      "300: reward 15.0 162.7 200.0, loss -0.5 -0.0 0.1\n",
      "350: reward 72.0 185.7 200.0, loss -0.3 -0.0 0.1\n",
      "400: reward 54.0 166.5 200.0, loss -0.3 0.0 0.2\n",
      "450: reward 200.0 200.0 200.0, loss -0.2 -0.0 0.1\n"
     ]
    }
   ],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "# 出力用にactionの修正値を計算\n",
    "# アクションは-10～10の範囲をとるが、学習は-1～1の範囲と仮定し、\n",
    "# 出力時に-10～10に戻す\n",
    "action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "action_scale = env.action_space.high - action_centor\n",
    "\n",
    "# print(env.action_space.high, env.action_space.low)\n",
    "# print(action_centor)\n",
    "# print(action_scale)\n",
    "# print(env.action_space)\n",
    "# モデルを作成\n",
    "# model = PolicyModel(env.action_space)\n",
    "# 出力サイズは1じゃだめ?\n",
    "model = PolicyModel(1)\n",
    "\n",
    "# 経験バッファ用\n",
    "experiences = []\n",
    "\n",
    "# 記録用\n",
    "history_metrics = []\n",
    "history_rewards = []\n",
    "\n",
    "interval = 50\n",
    "\n",
    "# 学習ループ\n",
    "for episode in range(500):\n",
    "    state = np.asarray(env.reset())\n",
    "#     print(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    episode_metrics = []\n",
    "\n",
    "    # 1episode\n",
    "    while not done:\n",
    "        # アクションを決定\n",
    "        action = model.sample_action(state)\n",
    "\n",
    "        # 1step進める（アクション値を修正して渡す）\n",
    "        n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "        n_state = np.asarray(n_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 経験を追加\n",
    "        experiences.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"n_state\": n_state,\n",
    "            \"done\": done,\n",
    "        })\n",
    "        state = n_state\n",
    "\n",
    "    #------------------------\n",
    "    # 1episode毎に学習する手法\n",
    "    #------------------------\n",
    "    metrics = train(model, experiences)\n",
    "    episode_metrics.append(metrics)\n",
    "    experiences.clear()  # 戦略が変わるので初期化\n",
    "\n",
    "    # メトリクス\n",
    "    if len(episode_metrics) == 0:\n",
    "        history_metrics.append((None, None, None, None))\n",
    "    else:\n",
    "        history_metrics.append(np.mean(episode_metrics, axis=0))  # 平均を保存\n",
    "\n",
    "    # 報酬\n",
    "    history_rewards.append(total_reward)\n",
    "    if episode % interval == 0:\n",
    "        print(\"{}: reward {:.1f} {:.1f} {:.1f}, loss {:.1f} {:.1f} {:.1f}\".format(\n",
    "            episode,\n",
    "            min(history_rewards[-interval:]), \n",
    "            np.mean(history_rewards[-interval:]), \n",
    "            max(history_rewards[-interval:]),\n",
    "            min(history_metrics[-interval:]),\n",
    "            np.mean(history_metrics[-interval:]),\n",
    "            max(history_metrics[-interval:]),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
