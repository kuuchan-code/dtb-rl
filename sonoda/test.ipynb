{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オリジナルのCartpoleを継承\n",
    "class MyCartpole(CartPoleEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # action_space を連続空間に変更\n",
    "        self.action_space = spaces.Box(-self.force_mag, self.force_mag, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # 終了ターンのが実装内で定義されているので実装\n",
    "        self.step_count = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # 200ターンたったら終了する\n",
    "        self.step_count += 1\n",
    "        if self.step_count > 200:\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # 例外処理\n",
    "        if np.isnan(action):\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # アクションをclipして force の値にする\n",
    "        force = np.clip(action, -self.force_mag, self.force_mag)[0]\n",
    "\n",
    "        #--- 以下オリジナルのstepコードをコピペ ---\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"tf\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28, reward: 28.0\n"
     ]
    }
   ],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "env.reset()     # ゲームの初期化\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    step += 1\n",
    "env.close()\n",
    "\n",
    "print(f\"step: {step}, reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 独自のモデルを定義\n",
    "class PolicyModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.pi_mean = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "        self.pi_stddev = keras.layers.Dense(action_space, activation=\"linear\")\n",
    "\n",
    "        # optimizer もついでに定義しておく\n",
    "        self.optimizer = Adam(lr=0.01)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        # σ^2 > 0 になるように変換(指数関数)\n",
    "        stddev = tf.exp(stddev)\n",
    "\n",
    "        return mean, stddev\n",
    "\n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state):\n",
    "        # モデルから平均と分散を取得\n",
    "        mean, stddev = self(state.reshape((1,-1)))\n",
    "\n",
    "        # ガウス分布に従った乱数をだす\n",
    "        sampled_action = tf.random.normal(tf.shape(mean), mean=mean, stddev=stddev)\n",
    "        return sampled_action.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHistory(history_rewards, history_metrics):\n",
    "    # グラフで表示\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('episode')\n",
    "    ax1.grid()\n",
    "    ax1.plot(history_rewards, color=\"C0\", label=\"reward\")\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot([m for m in history_metrics], color=\"C1\", marker='.', label=\"loss\")\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト用?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testEnv(env, sample_action_func):\n",
    "    action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "    action_scale = env.action_space.high - action_centor\n",
    "    for episode in range(5):\n",
    "        state = np.asarray(env.reset())\n",
    "        env.render()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = sample_action_func(state)\n",
    "            n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "            env.render()\n",
    "            state = np.asarray(n_state)\n",
    "            step += 1\n",
    "            total_reward += reward\n",
    "        print(\"{}, step: {}, reward: {}\".format(episode, step, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率分布計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logpi(mean, stddev, action):\n",
    "    a1 = -0.5 * np.log(2*np.pi)\n",
    "    a2 = -tf.math.log(stddev)\n",
    "    a3 = -0.5 * (((action - mean) / stddev) ** 2)\n",
    "    return a1 + a2 + a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, experiences):\n",
    "    gamma = 0.9\n",
    "\n",
    "    # 各経験毎に価値を推定、後ろから計算\n",
    "    v_vals = []\n",
    "    r = 0\n",
    "    for e in reversed(experiences):\n",
    "        if e[\"done\"]:\n",
    "            # 終了時は次の状態がないので報酬のみ\n",
    "            r = e[\"reward\"]\n",
    "        else:\n",
    "            r = e[\"reward\"] + gamma * r\n",
    "        v_vals.append(r)\n",
    "    v_vals.reverse()  # 反転して元に戻す\n",
    "    v_vals = np.asarray(v_vals).reshape((-1, 1))  # 整形\n",
    "\n",
    "    states = np.asarray([e[\"state\"] for e in experiences])\n",
    "    actions = np.asarray([e[\"action\"] for e in experiences])\n",
    "\n",
    "    # baseline\n",
    "    v_vals -= np.mean(v_vals)\n",
    "\n",
    "    # shapeを念のため確認\n",
    "    assert v_vals.shape == (len(experiences), 1)\n",
    "\n",
    "    # 勾配を計算\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # モデルから値を取得\n",
    "        mean, stddev = model(states, training=True)\n",
    "\n",
    "        # log(π(a|s))を計算\n",
    "        logpi = compute_logpi(mean, stddev, actions)\n",
    "\n",
    "        # log(π(a|s)) * Q(s,a) を計算\n",
    "        policy_loss = logpi * v_vals\n",
    "\n",
    "        # ミニバッチ処理\n",
    "        loss = -tf.reduce_mean(policy_loss)\n",
    "\n",
    "    # 勾配を元にoptimizerでモデルを更新\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.] [-10.]\n",
      "[0.]\n",
      "[10.]\n",
      "Box(-10.0, 10.0, (1,), float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Box'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# モデルを作成\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPolicyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 経験バッファ用\u001b[39;00m\n\u001b[1;32m     17\u001b[0m experiences \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mPolicyModel.__init__\u001b[0;34m(self, action_space)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense1 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_mean \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_stddev \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(action_space, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# optimizer もついでに定義しておく\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/layers/core.py:1160\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1146\u001b[0m              units,\n\u001b[1;32m   1147\u001b[0m              activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1155\u001b[0m              bias_constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1157\u001b[0m   \u001b[38;5;28msuper\u001b[39m(Dense, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1158\u001b[0m       activity_regularizer\u001b[38;5;241m=\u001b[39mactivity_regularizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1160\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(units, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m units\n\u001b[1;32m   1161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived an invalid value for `units`, expected \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma positive integer, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Box'"
     ]
    }
   ],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "# 出力用にactionの修正値を計算\n",
    "# アクションは-10～10の範囲をとるが、学習は-1～1の範囲と仮定し、\n",
    "# 出力時に-10～10に戻す\n",
    "action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "action_scale = env.action_space.high - action_centor\n",
    "\n",
    "print(env.action_space.high, env.action_space.low)\n",
    "print(action_centor)\n",
    "print(action_scale)\n",
    "print(env.action_space)\n",
    "# モデルを作成\n",
    "model = PolicyModel(env.action_space)\n",
    "\n",
    "# 経験バッファ用\n",
    "experiences = []\n",
    "\n",
    "# 記録用\n",
    "history_metrics = []\n",
    "history_rewards = []\n",
    "\n",
    "# 学習ループ\n",
    "for episode in range(500):\n",
    "    state = np.asarray(env.reset())\n",
    "#     print(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    episode_metrics = []\n",
    "\n",
    "    # 1episode\n",
    "    while not done:\n",
    "        # アクションを決定\n",
    "        action = model.sample_action(state)\n",
    "\n",
    "        # 1step進める（アクション値を修正して渡す）\n",
    "        n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "        n_state = np.asarray(n_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 経験を追加\n",
    "        experiences.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"n_state\": n_state,\n",
    "            \"done\": done,\n",
    "        })\n",
    "        state = n_state\n",
    "\n",
    "    #------------------------\n",
    "    # 1episode毎に学習する手法\n",
    "    #------------------------\n",
    "    metrics = train(model, experiences)\n",
    "    episode_metrics.append(metrics)\n",
    "    experiences.clear()  # 戦略が変わるので初期化\n",
    "\n",
    "    # メトリクス\n",
    "    if len(episode_metrics) == 0:\n",
    "        history_metrics.append((None, None, None, None))\n",
    "    else:\n",
    "        history_metrics.append(np.mean(episode_metrics, axis=0))  # 平均を保存\n",
    "\n",
    "    # 報酬\n",
    "    history_rewards.append(total_reward)\n",
    "    interval = 50\n",
    "    if episode % interval == 0:\n",
    "        print(\"{}: reward {:.1f} {:.1f} {:.1f}, loss {:.1f} {:.1f} {:.1f}\".format(\n",
    "            episode,\n",
    "            min(history_rewards[-interval:]), \n",
    "            np.mean(history_rewards[-interval:]), \n",
    "            max(history_rewards[-interval:]),\n",
    "            min(history_metrics[-interval:]),\n",
    "            np.mean(history_metrics[-interval:]),\n",
    "            max(history_metrics[-interval:]),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
