{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz55Pn6Zv4-9"
   },
   "source": [
    "# 0.表示用の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tAp1naUv8Mo",
    "outputId": "0a0176d9-1ff6-49d2-fbe2-fed3b8117cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Err:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:8 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists... Done\n",
      "\u001b[1;33mW: \u001b[0mGPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mThe repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease' is no longer signed.\u001b[0m\n",
      "\u001b[33mN: \u001b[0mUpdating from such a repository can't be done securely, and is therefore disabled by default.\u001b[0m\n",
      "\u001b[33mN: \u001b[0mSee apt-secure(8) manpage for repository creation and user configuration details.\u001b[0m\n",
      "Requirement already satisfied: gym-notebook-wrapper in /usr/local/lib/python3.8/dist-packages (1.3.2)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from gym-notebook-wrapper) (0.21.0)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from gym-notebook-wrapper) (8.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gym-notebook-wrapper) (3.5.1)\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.8/dist-packages (from gym-notebook-wrapper) (3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-notebook-wrapper) (1.22.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-notebook-wrapper) (2.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (0.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (0.1.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (60.7.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (0.1.3)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (5.1.1)\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (22.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (3.0.26)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (5.1.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (0.2.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->gym-notebook-wrapper) (2.11.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (9.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (4.29.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gym-notebook-wrapper) (21.3)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->gym-notebook-wrapper) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->gym-notebook-wrapper) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->gym-notebook-wrapper) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->gym-notebook-wrapper) (1.14.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (2.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (8.0.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (2.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (4.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from black->ipython->gym-notebook-wrapper) (0.4.3)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->gym-notebook-wrapper) (0.2.2)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->gym-notebook-wrapper) (0.8.2)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->gym-notebook-wrapper) (2.0.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 参考：https://qiita.com/ymd_h/items/c393797deb72e1779269\n",
    "!apt update && apt install xvfb\n",
    "!pip install gym-notebook-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CLNaiG9Wx1Mj"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Xvfb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgnwrapper\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgnwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoopAnimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m o \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gnwrapper/__init__.py:132\u001b[0m, in \u001b[0;36mLoopAnimation.__init__\u001b[0;34m(self, env, size)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,env,size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m768\u001b[39m)):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Wrap environment for Notebook\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        Virtual display size, whose default is (1024, 768)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_img \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gnwrapper/__init__.py:62\u001b[0m, in \u001b[0;36mVirtualDisplay.__init__\u001b[0;34m(self, env, size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display \u001b[38;5;241m=\u001b[39m \u001b[43m_VirtualDisplaySingleton\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gnwrapper/__init__.py:36\u001b[0m, in \u001b[0;36m_VirtualDisplaySingleton.__init__\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_display\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     original \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPopen\u001b[39m(cmd,pass_fds,stdout,stderr,shell):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyvirtualdisplay/display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyvirtualdisplay/xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[0;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyvirtualdisplay/abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyvirtualdisplay/util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb'"
     ]
    }
   ],
   "source": [
    "# 確認用\n",
    "\n",
    "import gnwrapper\n",
    "import gym\n",
    "\n",
    "env = gnwrapper.LoopAnimation(gym.make('CartPole-v1'))\n",
    "\n",
    "o = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    o, r, d, i = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if d:\n",
    "        env.reset()\n",
    "\n",
    "env.close()\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jGKDWy8_yR4"
   },
   "source": [
    "# 1.使用する環境(MyCartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqeJb4ZYNqco"
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# オリジナルのCartpoleを継承\n",
    "class MyCartpole(CartPoleEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # action_space を連続空間に変更\n",
    "        self.action_space = spaces.Box(-self.force_mag, self.force_mag, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # 終了ターンのが実装内で定義されているので実装\n",
    "        self.step_count = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        # 200ターンたったら終了する\n",
    "        self.step_count += 1\n",
    "        if self.step_count > 200:\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # 例外処理\n",
    "        if np.isnan(action):\n",
    "            return np.array(self.state), 0.0, True, {}\n",
    "        \n",
    "        # アクションをclipして force の値にする\n",
    "        force = np.clip(action, -self.force_mag, self.force_mag)[0]\n",
    "\n",
    "        #--- 以下オリジナルのstepコードをコピペ ---\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URB8EiPVuLyZ",
    "outputId": "65e57709-6d3b-4a1d-a727-8d004ea01ad2"
   },
   "outputs": [],
   "source": [
    "env = MyCartpole()\n",
    "env = gnwrapper.LoopAnimation(env)\n",
    "\n",
    "env.reset()     # ゲームの初期化\n",
    "env.render()    # ゲームの描画\n",
    "done = False\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "  action = env.action_space.sample()\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  total_reward += reward\n",
    "  step += 1\n",
    "  env.render()\n",
    "env.close()\n",
    "\n",
    "print(\"step: {}, reward: {}\".format(step, total_reward))\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTLwTKc6_58-"
   },
   "source": [
    "# 2.共通コード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0NFVxfsP2NW"
   },
   "source": [
    "## 2-1. 学習結果表示用コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfdPa47hP7Yn"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def drawHistory(history_rewards, history_metrics):\n",
    "  # グラフで表示\n",
    "  fig, ax1 = plt.subplots()\n",
    "\n",
    "  ax1.set_xlabel('episode')\n",
    "  ax1.grid()\n",
    "  ax1.plot(history_rewards, color=\"C0\", label=\"reward\")\n",
    "  ax1.legend(loc='upper left')\n",
    "\n",
    "  ax2 = ax1.twinx()\n",
    "  ax2.plot([m for m in history_metrics], color=\"C1\", marker='.', label=\"loss\")\n",
    "  ax2.legend(loc='upper right')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Asa-VTn5QJph"
   },
   "source": [
    "## 2-2. テスト用コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4xq5ImHQQPW"
   },
   "outputs": [],
   "source": [
    "def testEnv(env, sample_action_func):\n",
    "  action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "  action_scale = env.action_space.high - action_centor\n",
    "  for episode in range(5):\n",
    "    state = np.asarray(env.reset())\n",
    "    env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while not done:\n",
    "      action = sample_action_func(state)\n",
    "      n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "      env.render()\n",
    "      state = np.asarray(n_state)\n",
    "      step += 1\n",
    "      total_reward += reward\n",
    "    print(\"{}, step: {}, reward: {}\".format(episode, step, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOIdxC9dR1ht"
   },
   "source": [
    "## 2-3. テスト用コード(表示用)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WTPr0bRR5QB"
   },
   "outputs": [],
   "source": [
    "import gnwrapper\n",
    "def testRender(env, sample_action_func):\n",
    "  action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "  action_scale = env.action_space.high - action_centor\n",
    "  env = gnwrapper.LoopAnimation(env)\n",
    "  state = np.asarray(env.reset())\n",
    "  env.render()    # ゲームの描画\n",
    "  done = False\n",
    "  total_reward = 0\n",
    "  step = 0\n",
    "  while not done:\n",
    "    action = sample_action_func(state)\n",
    "    n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "    env.render()\n",
    "    state = np.asarray(n_state)\n",
    "    step += 1\n",
    "    total_reward += reward\n",
    "\n",
    "  print(\"step: {}, reward: {}\".format(step, total_reward))\n",
    "  env.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyUFmkLAGz75"
   },
   "source": [
    "# 方策の確率分布を計算する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VH6qT04vG0XJ"
   },
   "outputs": [],
   "source": [
    "def compute_logpi(mean, stddev, action):\n",
    "    a1 = -0.5 * np.log(2*np.pi)\n",
    "    a2 = -tf.math.log(stddev)\n",
    "    a3 = -0.5 * (((action - mean) / stddev) ** 2)\n",
    "    return a1 + a2 + a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qHUODal_7qw"
   },
   "source": [
    "# 3.方策勾配法(連続行動空間:ガウス分布)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh0j54116hGL"
   },
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txWEJhBJ_9QO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 独自のモデルを定義\n",
    "class PolicyModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.pi_mean = keras.layers.Dense(action_space.shape[0], activation=\"linear\")\n",
    "        self.pi_stddev = keras.layers.Dense(action_space.shape[0], activation=\"linear\")\n",
    "\n",
    "        # optimizer もついでに定義しておく\n",
    "        self.optimizer = Adam(lr=0.01)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False):\n",
    "        # 共通層\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # ガウス分布のパラメータ層\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        # σ > 0 になるように変形(指数関数)\n",
    "        stddev = tf.exp(stddev)\n",
    "\n",
    "        return mean, stddev\n",
    "    \n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state):\n",
    "        # モデルから平均と標準偏差を取得\n",
    "        mean, stddev = self(state.reshape((1,-1)))\n",
    "\n",
    "        # ガウス分布に従った乱数をだす\n",
    "        sampled_action = tf.random.normal(tf.shape(mean), mean=mean, stddev=stddev)\n",
    "        return sampled_action.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmHbdEsZpTWd"
   },
   "source": [
    "## 学習コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lo16bDeFth7"
   },
   "outputs": [],
   "source": [
    "def train(model, experiences):\n",
    "    gamma = 0.9\n",
    "\n",
    "    # 各経験毎に価値を推定、後ろから計算\n",
    "    v_vals = []\n",
    "    r = 0\n",
    "    for e in reversed(experiences):\n",
    "        if e[\"done\"]:\n",
    "            # 終了時は次の状態がないので報酬のみ\n",
    "            r = e[\"reward\"]\n",
    "        else:\n",
    "            r = e[\"reward\"] + gamma * r\n",
    "        v_vals.append(r)\n",
    "    v_vals.reverse()  # 反転して元に戻す\n",
    "    v_vals = np.asarray(v_vals).reshape((-1, 1))  # 整形\n",
    "\n",
    "    states = np.asarray([e[\"state\"] for e in experiences])\n",
    "    actions = np.asarray([e[\"action\"] for e in experiences])\n",
    "\n",
    "    # baseline\n",
    "    v_vals -= np.mean(v_vals)\n",
    "\n",
    "    # shapeを念のため確認\n",
    "    assert v_vals.shape == (len(experiences), 1)\n",
    "\n",
    "    # 勾配を計算\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # モデルから値を取得\n",
    "        mean, stddev = model(states, training=True)\n",
    "\n",
    "        # log(π(a|s))を計算\n",
    "        logpi = compute_logpi(mean, stddev, actions)\n",
    "\n",
    "        # log(π(a|s)) * Q(s,a) を計算\n",
    "        policy_loss = logpi * v_vals\n",
    "\n",
    "        # ミニバッチ処理\n",
    "        loss = -tf.reduce_mean(policy_loss)\n",
    "\n",
    "    # 勾配を元にoptimizerでモデルを更新\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BggF8rACpX9b"
   },
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtobZei_FFCf",
    "outputId": "ac8599bf-19ac-4d97-bc2a-992413ecb2ac"
   },
   "outputs": [],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "# 出力用にactionの修正値を計算\n",
    "# アクションは-10～10の範囲をとるが、学習は-1～1の範囲と仮定し、\n",
    "# 出力時に-10～10に戻す\n",
    "action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "action_scale = env.action_space.high - action_centor\n",
    "\n",
    "# モデルを作成\n",
    "model = PolicyModel(env.action_space)\n",
    "\n",
    "# 経験バッファ用\n",
    "experiences = []\n",
    "\n",
    "# 記録用\n",
    "history_metrics = []\n",
    "history_rewards = []\n",
    "\n",
    "# 学習ループ\n",
    "for episode in range(500):\n",
    "    state = np.asarray(env.reset())\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    episode_metrics = []\n",
    "\n",
    "    # 1episode\n",
    "    while not done:\n",
    "        # アクションを決定\n",
    "        action = model.sample_action(state)\n",
    "\n",
    "        # 1step進める（アクション値を修正して渡す）\n",
    "        n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "        n_state = np.asarray(n_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 経験を追加\n",
    "        experiences.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"n_state\": n_state,\n",
    "            \"done\": done,\n",
    "        })\n",
    "        state = n_state\n",
    "\n",
    "    #------------------------\n",
    "    # 1episode毎に学習する手法\n",
    "    #------------------------\n",
    "    metrics = train(model, experiences)\n",
    "    episode_metrics.append(metrics)\n",
    "    experiences.clear()  # 戦略が変わるので初期化\n",
    "\n",
    "    # メトリクス\n",
    "    if len(episode_metrics) == 0:\n",
    "        history_metrics.append((None, None, None, None))\n",
    "    else:\n",
    "        history_metrics.append(np.mean(episode_metrics, axis=0))  # 平均を保存\n",
    "\n",
    "    # 報酬\n",
    "    history_rewards.append(total_reward)\n",
    "    interval = 50\n",
    "    if episode % interval == 0:\n",
    "        print(\"{}: reward {:.1f} {:.1f} {:.1f}, loss {:.1f} {:.1f} {:.1f}\".format(\n",
    "            episode,\n",
    "            min(history_rewards[-interval:]), \n",
    "            np.mean(history_rewards[-interval:]), \n",
    "            max(history_rewards[-interval:]),\n",
    "            min(history_metrics[-interval:]),\n",
    "            np.mean(history_metrics[-interval:]),\n",
    "            max(history_metrics[-interval:]),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD5XqlnB9eyV"
   },
   "source": [
    "## 学習過程のグラフ表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Wm6b875u9dml",
    "outputId": "83cc58f6-2dea-4af8-d417-e040d35e5d86"
   },
   "outputs": [],
   "source": [
    "drawHistory(history_rewards, history_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpWuKrN49odT"
   },
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "id": "2d2pJF_N9oOh",
    "outputId": "c0c51ecd-fc17-4e46-cf2c-64b589e5831b"
   },
   "outputs": [],
   "source": [
    "testEnv(env, lambda state: model.sample_action(state))\n",
    "testRender(env, lambda state: model.sample_action(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBTS7RqxkFJ_"
   },
   "source": [
    "# 4.方策勾配法(連続行動空間:ガウス分布(tanh適用))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFM-jBhukFKI"
   },
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYwy8UCskFKI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 独自のモデルを定義\n",
    "class PolicyModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(16, activation=\"relu\")\n",
    "        self.pi_mean = keras.layers.Dense(action_space.shape[0], activation=\"linear\")\n",
    "        self.pi_stddev = keras.layers.Dense(action_space.shape[0], activation=\"linear\")\n",
    "\n",
    "        # optimizer もついでに定義しておく\n",
    "        self.optimizer = Adam(lr=0.01)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False):\n",
    "        # 共通層\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        # ガウス分布のパラメータ層\n",
    "        mean = self.pi_mean(x)\n",
    "        stddev = self.pi_stddev(x)\n",
    "\n",
    "        # σ > 0 になるように変形(指数関数)\n",
    "        stddev = tf.exp(stddev)\n",
    "\n",
    "        return mean, stddev\n",
    "    \n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state):\n",
    "        # モデルから平均と標準偏差を取得\n",
    "        mean, stddev = self(state.reshape((1,-1)))\n",
    "\n",
    "        # ガウス分布に従った乱数をだす\n",
    "        actions = tf.random.normal(tf.shape(mean), mean=mean, stddev=stddev)\n",
    "\n",
    "        # tanhを適用\n",
    "        actions_squashed = tf.tanh(actions)\n",
    "\n",
    "        # 学習にtanh適用前のactionも欲しいのでそれも返す\n",
    "        return actions_squashed.numpy()[0], actions.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCUCI-1ikFKJ"
   },
   "source": [
    "## 学習コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXFJw6VAkFKJ"
   },
   "outputs": [],
   "source": [
    "def train(model, experiences):\n",
    "    gamma = 0.9\n",
    "\n",
    "    # 各経験毎に価値を推定、後ろから計算\n",
    "    v_vals = []\n",
    "    r = 0\n",
    "    for e in reversed(experiences):\n",
    "        if e[\"done\"]:\n",
    "            # 終了時は次の状態がないので報酬のみ\n",
    "            r = e[\"reward\"]\n",
    "        else:\n",
    "            r = e[\"reward\"] + gamma * r\n",
    "        v_vals.append(r)\n",
    "    v_vals.reverse()  # 反転して元に戻す\n",
    "    v_vals = np.asarray(v_vals).reshape((-1, 1))  # 整形\n",
    "\n",
    "    states = np.asarray([e[\"state\"] for e in experiences])\n",
    "    action_org = np.asarray([e[\"action_org\"] for e in experiences])\n",
    "\n",
    "    # baseline\n",
    "    v_vals -= np.mean(v_vals)\n",
    "\n",
    "    # shapeを念のため確認\n",
    "    assert v_vals.shape == (len(experiences), 1)\n",
    "\n",
    "    # 勾配を計算\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # モデルから値を取得\n",
    "        mean, stddev = model(states, training=True)\n",
    "\n",
    "        # log(μ(a|s))を計算\n",
    "        logmu = compute_logpi(mean, stddev, action_org)\n",
    "\n",
    "        # log(π(a|s))を計算\n",
    "        tmp = 1 - tf.tanh(action_org) ** 2\n",
    "        tmp = tf.clip_by_value(tmp, 1e-10, 1.0)  # log(0)回避用\n",
    "        logpi = logmu - tf.reduce_sum(tf.math.log(tmp), axis=1, keepdims=True)\n",
    "\n",
    "        # log(π(a|s)) * Q(s,a) を計算\n",
    "        policy_loss = logpi * v_vals\n",
    "\n",
    "        # ミニバッチ処理\n",
    "        loss = -tf.reduce_mean(policy_loss)\n",
    "\n",
    "    # 勾配を元にoptimizerでモデルを更新\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBgngkedkFKJ"
   },
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWw6X5uvkFKK",
    "outputId": "817bd5eb-ec52-4117-b1de-5c09599e6d9b"
   },
   "outputs": [],
   "source": [
    "env = MyCartpole()\n",
    "\n",
    "# 出力用にactionの逆正規化値を計算\n",
    "# アクションは-10～10の範囲をとるが、それだと安定しないので-1～1の範囲の出力を想定し、\n",
    "# 出力時に-10～10に戻す\n",
    "action_centor = (env.action_space.high + env.action_space.low)/2\n",
    "action_scale = env.action_space.high - action_centor\n",
    "\n",
    "# モデルを作成\n",
    "model = PolicyModel(env.action_space)\n",
    "\n",
    "# 経験バッファ用\n",
    "experiences = []\n",
    "\n",
    "# 記録用\n",
    "history_metrics = []\n",
    "history_rewards = []\n",
    "\n",
    "# 学習ループ\n",
    "for episode in range(500):\n",
    "    state = np.asarray(env.reset())\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    episode_metrics = []\n",
    "\n",
    "    # 1episode\n",
    "    while not done:\n",
    "        # アクションを決定\n",
    "        action, action_org = model.sample_action(state)\n",
    "\n",
    "        # 1step進める（アクション値を修正して渡す）\n",
    "        n_state, reward, done, _ = env.step(action * action_scale + action_centor)\n",
    "        n_state = np.asarray(n_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        # 経験を追加\n",
    "        experiences.append({\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"action_org\": action_org,  # 追加\n",
    "            \"reward\": reward,\n",
    "            \"n_state\": n_state,\n",
    "            \"done\": done,\n",
    "        })\n",
    "        state = n_state\n",
    "\n",
    "    #------------------------\n",
    "    # 1episode毎に学習する手法\n",
    "    #------------------------\n",
    "    metrics = train(model, experiences)\n",
    "    episode_metrics.append(metrics)\n",
    "    experiences.clear()  # 戦略が変わるので初期化\n",
    "\n",
    "    # メトリクス\n",
    "    if len(episode_metrics) == 0:\n",
    "        history_metrics.append((None, None, None, None))\n",
    "    else:\n",
    "        history_metrics.append(np.mean(episode_metrics, axis=0))  # 平均を保存\n",
    "\n",
    "    # 報酬\n",
    "    history_rewards.append(total_reward)\n",
    "    interval = 50\n",
    "    if episode % interval == 0:\n",
    "        print(\"{}: reward {:.1f} {:.1f} {:.1f}, loss {:.1f} {:.1f} {:.1f}\".format(\n",
    "            episode, \n",
    "            min(history_rewards[-interval:]), \n",
    "            np.mean(history_rewards[-interval:]), \n",
    "            max(history_rewards[-interval:]),\n",
    "            min(history_metrics[-interval:]),\n",
    "            np.mean(history_metrics[-interval:]),\n",
    "            max(history_metrics[-interval:]),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoSOWRtzkFKK"
   },
   "source": [
    "## 学習過程のグラフ表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "vn8amH1AkFKL",
    "outputId": "dcd8eadc-2888-4695-e78c-9c23b2a71e76"
   },
   "outputs": [],
   "source": [
    "drawHistory(history_rewards, history_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYNtOVrHkFKL"
   },
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "id": "t5v2AiLDkFKL",
    "outputId": "2eb190df-ea8f-4034-def7-bb90b47e6614"
   },
   "outputs": [],
   "source": [
    "def sample_action(state):\n",
    "  action, _ = model.sample_action(state)\n",
    "  return action\n",
    "testEnv(env, sample_action)\n",
    "testRender(env, sample_action)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jz55Pn6Zv4-9",
    "5jGKDWy8_yR4",
    "MTLwTKc6_58-",
    "H0NFVxfsP2NW",
    "Asa-VTn5QJph",
    "NOIdxC9dR1ht",
    "0qHUODal_7qw",
    "Jh0j54116hGL",
    "ZmHbdEsZpTWd",
    "QBTS7RqxkFJ_",
    "ZFM-jBhukFKI",
    "OCUCI-1ikFKJ"
   ],
   "name": "qiita44_Action2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
